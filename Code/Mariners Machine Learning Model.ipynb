{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David C. Buehler\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../Data/model_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is dedicated to tuning the hyperparameters of the XGBoost model for use in the main Challenge notebook. Here we will tune 3 hyperparameters: n_estimators, max_depth, and learning rate. I chose these parameters in specific, because in doing some research, these were the common parameters tuned for a classification problem. Seeing how this is a very large dataset, we're also going to be using the subsample parameter and set that to train on 33% of the dataset (instances selected randomly). This does trade some bias for variance, but it also significantly speeds up training time which is what we're after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['pitcher_id', 'batter_id', 'stadium_id', 'umpire_id', 'catcher_id', 'pitch_call', 'is_swing', 'pitch_id', 'date'], axis=1)\n",
    "y = train_df['is_swing']\n",
    "\n",
    "X = pd.get_dummies(X, prefix=['level', 'pitcher', 'batter', 'is'], columns=['level', 'pitcher_side', 'batter_side', 'pitch_type'], dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__n_estimators': 1500} 0.7826578143525036\n"
     ]
    }
   ],
   "source": [
    "steps = [('xgb', XGBClassifier(seed=34, subsample=.33))]\n",
    "param_grid = {'xgb__n_estimators': np.arange(500, 1600, 100)}\n",
    "pipeline = Pipeline(steps)\n",
    "cv_1 = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "cv_1.fit(X, y)\n",
    "print(cv_1.best_params_, cv_1.best_score_)\n",
    "n_estimators = cv_1.best_params_['xgb__n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__max_depth': 5} 0.7830036649737934\n"
     ]
    }
   ],
   "source": [
    "steps = [('xgb', XGBClassifier(n_estimators=n_estimators, seed=34, subsample=.33))]\n",
    "param_grid = {'xgb__max_depth': np.arange(3,10,2)}\n",
    "pipeline = Pipeline(steps)\n",
    "cv_2 = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "cv_2.fit(X, y)\n",
    "print(cv_2.best_params_, cv_2.best_score_)\n",
    "max_depth = cv_2.best_params_['xgb__max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgb__learning_rate': 0.05} 0.7841352088736265\n"
     ]
    }
   ],
   "source": [
    "steps = [('xgb', XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, seed=34, subsample=.33))]\n",
    "param_grid = {'xgb__learning_rate': np.arange(0.05, 0.35, .05)}\n",
    "pipeline = Pipeline(steps)\n",
    "cv_3 = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "cv_3.fit(X, y)\n",
    "print(cv_3.best_params_, cv_3.best_score_)\n",
    "learning_rate = cv_3.best_params_['xgb__learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(learning_rate=0.05, max_depth=5, n_estimators=1500, seed=34)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, seed=34)\n",
    "xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model with its optimal hyperparameters and fitted, we'll save this model for use in the other notebook to look at metrics on it, as well as make predictions on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuned_xgboost_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filename = 'tuned_xgboost_model.pkl'\n",
    "dump(xgb, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
